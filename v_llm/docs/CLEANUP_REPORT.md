# Rapport de nettoyage du projet BioMistral

## ğŸ“Š Ã‰tat actuel du workflow

### Workflow de production (Ã  conserver)
1. **GÃ©nÃ©ration dataset** â†’ `generate_finetune_dataset.py` âœ…
2. **Fine-tuning** â†’ `finetune_biomistral_unsloth.py` (Colab) âœ…
3. **Merge LoRA** â†’ `merge_biomistral.py` âœ…
4. **Conversion GGUF** â†’ `convert-hf-to-gguf.py` âœ…
5. **Indexation RAG** â†’ `indexage.py` âœ…
6. **Interface Ollama** â†’ `ollama.py` âœ…
7. **CLI interactive** â†’ `main.py` âœ…
8. **Ã‰valuation** â†’ `evaluate_model.py` âœ…

### DonnÃ©es essentielles (Ã  conserver)
- `guidelines.json` - Directives mÃ©dicales
- `clinical_cases_train.jsonl` - Dataset d'entraÃ®nement (160 exemples)
- `clinical_cases_val.jsonl` - Dataset de validation (40 exemples)
- `Modelfile` - Configuration Ollama
- `requirements.txt` - DÃ©pendances Python

---

## ğŸ—‘ï¸ Fichiers Ã  supprimer

### 1. Scripts obsolÃ¨tes
- âŒ **`prepare_finetuning_data.py`** (213 lignes)
  - RemplacÃ© par `generate_finetune_dataset.py`
  - GÃ©nÃ©rait seulement ~30 exemples vs 200 maintenant
  - Format d'instruction obsolÃ¨te

### 2. Dossiers temporaires de build
- âŒ **`llama.cpp-temp/`** (clone complet llama.cpp)
  - UtilisÃ© uniquement pour conversion GGUF
  - Seul `gguf-py/` est nÃ©cessaire
  - OccupÃ© plusieurs centaines de MB

### 3. ModÃ¨les intermÃ©diaires non utilisÃ©s
- âŒ **`biomistral_clinical_lora_v2_merged-f16.gguf`** (~14 GB)
  - Format F16 non quantifiÃ©, trÃ¨s lourd
  - `biomistral_clinical_lora_v2_merged-q8_0.gguf` suffit (Q8 balance taille/qualitÃ©)
  - F16 conservÃ© uniquement si besoin de conversion ultÃ©rieure

### 4. Checkpoints intermÃ©diaires de training
- âŒ **`biomistral_clinical_lora_v2/checkpoint-20/`**
- âŒ **`biomistral_clinical_lora_v2/checkpoint-40/`**
- âš ï¸ **`biomistral_clinical_lora_v2/checkpoint-60/`** (garder si c'est le meilleur)
  - Ne conserver que le checkpoint final utilisÃ© pour le merge

### 5. Documentation obsolÃ¨te
- âš ï¸ **`FINE_TUNING_GUIDE.md`**
  - Ã€ vÃ©rifier si contient des infos non documentÃ©es ailleurs
  - Sinon peut Ãªtre archivÃ©

### 6. Cache Python
- âŒ **`__pycache__/`**
  - RÃ©gÃ©nÃ©rÃ© automatiquement
  - AjoutÃ© au `.gitignore`

### 7. Base de donnÃ©es RAG locale
- âš ï¸ **`rag_db/`**
  - ChromaDB locale recrÃ©Ã©e Ã  chaque run
  - Peut Ãªtre supprimÃ©e (elle se reconstruit via `indexage.py`)

---

## ğŸ§¹ Code mort Ã  nettoyer dans les fichiers actifs

### `ollama.py` - Fonctions heuristiques redondantes

Le fichier contient **3 fonctions de fallback** qui ne sont plus nÃ©cessaires car le modÃ¨le fine-tunÃ© respecte maintenant le format :

```python
# LIGNES Ã€ SUPPRIMER (lignes ~112-300)

def _needs_clarification(case_text: str) -> bool:
    # Logique heuristique pour dÃ©tecter si clarification nÃ©cessaire
    # âŒ Non utilisÃ©e - le modÃ¨le dÃ©cide maintenant

def _heuristic_recommendation(case_text: str) -> str:
    # Logique heuristique pour recommandation d'imagerie
    # âŒ Non utilisÃ©e - le modÃ¨le gÃ©nÃ¨re les recommandations

def _extract_json(s: str):
    # Extraction JSON de rÃ©ponses structurÃ©es
    # âŒ Format JSON abandonnÃ© au profit de texte libre structurÃ©
```

**Impact** : RÃ©duction de ~180 lignes, simplification de la maintenance.

### `main.py` - Logique de parsing complexe

```python
# LIGNES Ã€ SIMPLIFIER (lignes ~40-100)

# Logique complexe de mapping rÃ©ponses â†’ questions avec follow-ups
# Si le modÃ¨le est assez robuste, cette complexitÃ© peut Ãªtre rÃ©duite
# en posant simplement les questions en sÃ©quence sans parsing sophistiquÃ©
```

**Recommandation** : Tester si le modÃ¨le fine-tunÃ© gÃ¨re bien les clarifications itÃ©ratives sans cette orchestration.

### `evaluate_model.py` - Imports redondants

```python
# LIGNE 4
from pathlib import Path

# LIGNE 8  
import importlib.util
```

Ces imports sont dÃ©jÃ  prÃ©sents, vÃ©rifier les doublons.

---

## ğŸ“ Actions recommandÃ©es

### Phase 1 : Suppression sans risque (immÃ©diat)
```bash
# Supprimer scripts obsolÃ¨tes
rm prepare_finetuning_data.py

# Supprimer dossier temporaire llama.cpp
rm -rf llama.cpp-temp/

# Supprimer checkpoints intermÃ©diaires (garder uniquement le final)
rm -rf biomistral_clinical_lora_v2/checkpoint-20/
rm -rf biomistral_clinical_lora_v2/checkpoint-40/

# Supprimer cache Python
rm -rf __pycache__/

# Supprimer base RAG locale (se reconstruit)
rm -rf rag_db/
```

### Phase 2 : Nettoyage conditionnel (aprÃ¨s vÃ©rification)
```bash
# Si F16 non nÃ©cessaire pour futures conversions
rm biomistral_clinical_lora_v2_merged-f16.gguf  # Ã‰conomise ~14 GB

# Si checkpoint-60 n'est pas le meilleur
rm -rf biomistral_clinical_lora_v2/checkpoint-60/
```

### Phase 3 : Refactoring code (aprÃ¨s tests)
1. Nettoyer `ollama.py` :
   - Supprimer `_needs_clarification()`
   - Supprimer `_heuristic_recommendation()`
   - Supprimer `_extract_json()`
   - Garder uniquement `_call_model()` et `rag_biomistral_query()`

2. Simplifier `main.py` :
   - RÃ©duire la logique de parsing si le modÃ¨le gÃ¨re bien les clarifications

3. VÃ©rifier imports dans `evaluate_model.py`

---

## ğŸ“¦ Structure finale recommandÃ©e

```
v_llm/
â”œâ”€â”€ core/                           # Scripts principaux
â”‚   â”œâ”€â”€ ollama.py                   # Interface Ollama (nettoyÃ©e)
â”‚   â”œâ”€â”€ indexage.py                 # Indexation RAG
â”‚   â””â”€â”€ main.py                     # CLI interactive
â”œâ”€â”€ training/                       # Scripts d'entraÃ®nement
â”‚   â”œâ”€â”€ generate_finetune_dataset.py
â”‚   â”œâ”€â”€ finetune_biomistral_unsloth.py
â”‚   â””â”€â”€ merge_biomistral.py
â”œâ”€â”€ conversion/                     # Conversion GGUF
â”‚   â”œâ”€â”€ convert-hf-to-gguf.py
â”‚   â””â”€â”€ gguf-py/
â”œâ”€â”€ evaluation/                     # Ã‰valuation
â”‚   â””â”€â”€ evaluate_model.py
â”œâ”€â”€ data/                           # DonnÃ©es
â”‚   â”œâ”€â”€ guidelines.json
â”‚   â”œâ”€â”€ clinical_cases_train.jsonl
â”‚   â””â”€â”€ clinical_cases_val.jsonl
â”œâ”€â”€ models/                         # ModÃ¨les finaux
â”‚   â”œâ”€â”€ biomistral_clinical_lora_v2/       # Adapters LoRA finaux
â”‚   â”œâ”€â”€ biomistral_clinical_lora_v2_merged/ # ModÃ¨le HF mergÃ©
â”‚   â””â”€â”€ biomistral_clinical_lora_v2_merged-q8_0.gguf  # GGUF Ollama
â”œâ”€â”€ Modelfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ’¾ Estimation du gain d'espace

| Ã‰lÃ©ment | Taille | Gain |
|---------|--------|------|
| `llama.cpp-temp/` | ~500 MB | âœ… |
| `biomistral_clinical_lora_v2_merged-f16.gguf` | ~14 GB | âš ï¸ (si non nÃ©cessaire) |
| Checkpoints intermÃ©diaires | ~2-3 GB | âœ… |
| `__pycache__/` | ~5 MB | âœ… |
| `rag_db/` | ~20 MB | âœ… |
| **Total minimum** | **~3 GB** | |
| **Total avec F16** | **~17 GB** | |

---

## âš ï¸ PrÃ©cautions

1. **Backup avant suppression** : 
   ```bash
   tar -czf v_llm_backup_$(date +%Y%m%d).tar.gz v_llm/
   ```

2. **Tester aprÃ¨s nettoyage** :
   ```bash
   python evaluate_model.py --limit 5
   python main.py
   ```

3. **Git tracking** : Si sous git, commit avant nettoyage pour rollback facile

4. **ModÃ¨les de base** : Ne PAS supprimer `/Users/alexpeirano/Desktop/commande_entreprise/BioMistral-7B` (hors projet)
